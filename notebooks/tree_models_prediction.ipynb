{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from xgboost import XGBRegressor\n",
    "from scipy import stats\n",
    "from utils import calculate_CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hospitalization data\n",
    "hosp = pd.read_csv(\"../data/COVID_hosp.csv\")\n",
    "hosp = hosp[hosp['geography'] == 'DE'][['date','total']]\n",
    "hosp = hosp.rename(columns={'total': 'hosp'})\n",
    "hosp['date'] = pd.to_datetime(hosp['date'])\n",
    "hosp = hosp.set_index('date', drop=True)\n",
    "# Calculate incidence per 100.000\n",
    "hosp = hosp['hosp'] / (84357 / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import virus load data (interpolated)\n",
    "virus = pd.read_excel(\"../data/amelag_aggregierte_kurve.xlsx\")\n",
    "virus = virus[[\"datum\", \"loess_vorhersage\"]].dropna()\n",
    "virus = virus.rename(columns={'datum': 'date', 'loess_vorhersage': 'virus'})\n",
    "virus['date'] = pd.to_datetime(virus['date'])\n",
    "virus = virus.set_index('date', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the two DataFrames on the 'date' column\n",
    "hosp_virus = pd.merge(virus, hosp, on='date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_virus['hosp'] = np.log(hosp_virus['hosp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_virus['hosp_shifted'] = hosp_virus['hosp'].shift(-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_virus['hosp_diff'] = hosp_virus['hosp'].diff()\n",
    "hosp_virus['hosp_diff_shifted'] = hosp_virus['hosp_diff'].shift(-7)\n",
    "\n",
    "hosp_virus = hosp_virus.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, train_size, test_size, shift_size):\n",
    "    \"\"\"\n",
    "    Split a given dataframe into smaller chunks with a constant train and test size.\n",
    "    The chunk window is moved forward by a specified shift value.\n",
    "    If the last test part of a chunk does not have the full test length, the chunk is discarded.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame, the dataframe to be split\n",
    "    - train_size: int, the length of the train window\n",
    "    - test_size: int, the length of the test window\n",
    "    - shift_size: int, the value by which the chunk window moves forward\n",
    "\n",
    "    Returns:\n",
    "    - chunks: list of tuples, each tuple contains a train window and its corresponding test window\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    # Determine the total length of the dataframe\n",
    "    total_length = len(df)\n",
    "\n",
    "    # Start index for the current chunk\n",
    "    start_index = 0\n",
    "\n",
    "    while start_index + train_size + test_size <= total_length:\n",
    "        # Extract the train and test data for the current chunk\n",
    "        train_data = df.iloc[start_index:start_index + train_size]\n",
    "        test_data = df.iloc[start_index + train_size:start_index + train_size + test_size]\n",
    "\n",
    "        # Add the train-test pair to the chunks list\n",
    "        chunks.append((train_data, test_data))\n",
    "\n",
    "        # Move the chunk window forward\n",
    "        start_index += shift_size\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_dataframe(hosp_virus, 70, 7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, chunk_train_data, is_autoregressive: bool, model_type: str, cv_type: str) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for Optuna.\n",
    "    Optimizes Hyperparameters for XBGoost or Random Forest model for a given Cross-Validation method.\n",
    "    Data can be autoregressive or also include the waste water data.\n",
    "    \n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): Optuna's trial object used for optimization.\n",
    "        chunk_train_data (pd.DataFrame): Training data chunk for the current iteration.\n",
    "        is_autoregressive (bool): Flag indicating whether the data is autoregressive.\n",
    "        model_type (str): Type of model to be optimized, either 'XGB' for XGBoost or 'RF' for Random Forest.\n",
    "        cv_type (str): Type of cross-validation method to be used.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean MAPE (Mean Absolute Percentage Error) across all cross-validation folds for the given hyperparameters.\n",
    "    \"\"\"\n",
    "    # Define feature and target columns based on is_autoregressive flag\n",
    "    TARGET_COLUMN = ['hosp_diff_shifted']\n",
    "    if is_autoregressive:\n",
    "        FEATURE_COLUMNS = ['hosp_diff']\n",
    "    else:\n",
    "        FEATURE_COLUMNS = ['virus', 'hosp_diff']\n",
    "\n",
    "\n",
    "    # Define model parameters based on model_type\n",
    "    if model_type == 'XGB':\n",
    "        params = {\n",
    "            'n_estimators': 200,\n",
    "            'random_state': 42,\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 12),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 10, log=True),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 10, log=True)\n",
    "        }\n",
    "        model = XGBRegressor(**params)\n",
    "    elif model_type == 'RF':\n",
    "        params = {\n",
    "            'n_jobs': -1, # Utilize all available CPU cores\n",
    "            'n_estimators': 200,\n",
    "            'random_state': 42,\n",
    "            'min_samples_split': trial.suggest_categorical('min_samples_split', [2, 4, 8, 16]),\n",
    "            'min_samples_leaf': trial.suggest_categorical('min_samples_leaf', [1, 2, 4]),\n",
    "        }\n",
    "        model = RandomForestRegressor(**params)\n",
    "        \n",
    "    # Define CV-method to be used\n",
    "    if cv_type == 'classical':\n",
    "        splitter = TimeSeriesSplit(n_splits=5)\n",
    "    if cv_type == 'blocked' or 'blocked_small':\n",
    "        chunk_splits = split_dataframe(chunk_train_data, int(len(chunk_train_data)*0.2), int(len(chunk_train_data)*0.2), int(len(chunk_train_data)*0.1))\n",
    "    if cv_type == 'weighted_linear' or cv_type == 'weighted_squared':\n",
    "        splitter = TimeSeriesSplit(n_splits=5)\n",
    "        weights = []\n",
    "    if cv_type == 'k_fold':\n",
    "        splitter = KFold(n_splits=5)\n",
    "    if cv_type == 'no_cv':\n",
    "        train_size = int(len(chunk_train_data) * 0.8)\n",
    "        train_no_cv = chunk_train_data.iloc[:train_size]\n",
    "        test_no_cv = chunk_train_data.iloc[train_size:]\n",
    "        chunk_splits = [(train_no_cv, test_no_cv)]\n",
    "\n",
    "    # Calculate average MAPE over all CV-folds (splits)\n",
    "    split_mapes = []\n",
    "    \n",
    "    if cv_type in ['classical', 'weighted_linear', 'weighted_squared', 'k_fold']:\n",
    "        for split_number, (train_idx, val_idx) in enumerate(splitter.split(chunk_train_data)):\n",
    "            train_data = chunk_train_data.iloc[train_idx]\n",
    "            val_data = chunk_train_data.iloc[val_idx]\n",
    "    \n",
    "            X_train, y_train = train_data[FEATURE_COLUMNS], train_data[TARGET_COLUMN]\n",
    "            X_test, y_test = val_data[FEATURE_COLUMNS], np.exp(val_data['hosp_shifted'])\n",
    "    \n",
    "    \n",
    "            y_train_df = y_train.copy()\n",
    "            if model_type == 'RF':\n",
    "                # Flatten the target variable if needed\n",
    "                y_train = np.ravel(y_train)\n",
    "    \n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "    \n",
    "            # Transform back\n",
    "            initial_value = hosp_virus.loc[y_train_df.last_valid_index(), 'hosp_shifted']\n",
    "            y_pred = initial_value + y_pred.cumsum()\n",
    "            y_pred = np.exp(y_pred)\n",
    "    \n",
    "            mape = mean_absolute_error(y_test, y_pred) / abs(y_test.mean()) * 100\n",
    "            split_mapes.append(mape)\n",
    "    \n",
    "            if cv_type == 'weighted_linear' or cv_type == 'weighted_squared':\n",
    "                weights.append(len(train_idx))\n",
    "    \n",
    "    if cv_type in ['blocked', 'blocked_small', 'no_cv']:\n",
    "        for split_number, (train_chunk, test_chunk) in enumerate(chunk_splits):\n",
    "            X_train, y_train = train_chunk[FEATURE_COLUMNS], train_chunk[TARGET_COLUMN]\n",
    "            X_test, y_test = test_chunk[FEATURE_COLUMNS], np.exp(test_chunk['hosp_shifted'])\n",
    "\n",
    "            y_train_df = y_train.copy()\n",
    "            if model_type == 'RF':\n",
    "                # Flatten the target variable if needed\n",
    "                y_train = np.ravel(y_train)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Transform back\n",
    "            initial_value = hosp_virus.loc[y_train_df.last_valid_index(), 'hosp_shifted']\n",
    "            y_pred = initial_value + y_pred.cumsum()\n",
    "            y_pred = np.exp(y_pred)\n",
    "\n",
    "            mape = mean_absolute_error(y_test, y_pred) / abs(y_test.mean()) * 100\n",
    "            split_mapes.append(mape)\n",
    "\n",
    "\n",
    "    # Return mean MAPE or weighted mean MAPE\n",
    "    if cv_type == 'weighted_linear':\n",
    "        linear_weights = [value / sum(weights) for value in weights]\n",
    "        return np.average(split_mapes, weights=linear_weights)\n",
    "    elif cv_type == 'weighted_squared':\n",
    "        squared_weights = [value**2 / sum([value**2 for value in weights]) for value in weights]\n",
    "        return np.average(split_mapes, weights=squared_weights)\n",
    "    else:\n",
    "        return np.mean(split_mapes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_chunks(is_autoregressive: bool, model_type: str, cv_type: str) -> None:\n",
    "    \"\"\"\n",
    "    Optimizes model hyperparameters for each chunk of data.\n",
    "\n",
    "    Args:\n",
    "        is_autoregressive (bool): Flag indicating whether the data is autoregressive.\n",
    "        model_type (str): Type of model to be optimized, either 'XGB' for XGBoost or 'RF' for Random Forest.\n",
    "        cv_type (str): Type of cross-validation method to be used.\n",
    "    \"\"\"\n",
    "    # Set the logging level of Optuna to ERROR\n",
    "    optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "    \n",
    "    # Reset the lists before starting a new optimization task\n",
    "    global best_chunk_params, cv_mapes, test_mapes, test_maes  # Declare global variables\n",
    "    best_chunk_params = []\n",
    "    cv_mapes = []\n",
    "    test_mapes = []\n",
    "    test_maes = []\n",
    "\n",
    "    for chunk_number, (chunk_train_data, chunk_test_data) in enumerate(chunks):\n",
    "        print(f'CURRENT CHUNK: {chunk_number}/{len(chunks)}')\n",
    "        \n",
    "        if model_type == 'XGB':\n",
    "            study = optuna.create_study(direction='minimize')\n",
    "        if model_type == 'RF':\n",
    "            # Define categorical parameters for Random Forest\n",
    "            param_grid = {\n",
    "                'min_samples_split': [2, 4, 8, 16],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "            }\n",
    "            study = optuna.create_study(direction='minimize', sampler=optuna.samplers.GridSampler(param_grid))\n",
    "            \n",
    "        objective_func = lambda trial: objective(trial, chunk_train_data, is_autoregressive, model_type, cv_type)\n",
    "        study.optimize(objective_func, n_trials=100) # Only used for TPE-Sampler \n",
    "\n",
    "        best_chunk_params.append(study.best_params)\n",
    "        cv_mapes.append(study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_mapes(best_chunk_params, is_autoregressive: bool, model_type: str, cv_type: str, is_cross_modal: bool) -> None:\n",
    "    \"\"\"\n",
    "    Calculate test MAPEs for each chunk using the best parameters found during optimization.\n",
    "\n",
    "    Args:\n",
    "        best_chunk_params (list): List of best parameters found for each chunk during optimization.\n",
    "        is_autoregressive (bool): Flag indicating whether the data is autoregressive.\n",
    "        model_type (str): Type of model to be used, either 'XGB' for XGBoost or 'RF' for Random Forest.\n",
    "    \"\"\"\n",
    "    \n",
    "    for chunk_number, (chunk_train_data, chunk_test_data) in enumerate(chunks):\n",
    "        print(f'CURRENT TEST CHUNK: {chunk_number}')\n",
    "\n",
    "        # Account for smaller train set in refit when cv method is blocked_small\n",
    "        if cv_type == 'blocked_small':\n",
    "            discard_size = int(len(chunk_train_data) * 0.8)\n",
    "            chunk_train_data = chunk_train_data.iloc[discard_size:]\n",
    "\n",
    "        best_params = best_chunk_params[chunk_number]  # Retrieve the best parameters for the current chunk\n",
    "\n",
    "        # Define feature and target columns based on is_autoregressive flag\n",
    "        TARGET_COLUMN = ['hosp_diff_shifted']\n",
    "        if is_autoregressive:\n",
    "            FEATURE_COLUMNS = ['hosp_diff']\n",
    "        elif is_cross_modal and not is_autoregressive:\n",
    "            print(\"cross_modal\")\n",
    "            FEATURE_COLUMNS = ['virus']\n",
    "        else:\n",
    "            FEATURE_COLUMNS = ['virus', 'hosp_diff']    \n",
    "\n",
    "        # Define model based on model_type\n",
    "        if model_type == 'XGB':\n",
    "            model = XGBRegressor(**best_params)\n",
    "        elif model_type == 'RF':\n",
    "            model = RandomForestRegressor(**best_params)\n",
    "\n",
    "        X_train = chunk_train_data[FEATURE_COLUMNS]\n",
    "        y_train = chunk_train_data[TARGET_COLUMN]\n",
    "        \n",
    "        X_test = chunk_test_data[FEATURE_COLUMNS]\n",
    "        y_test = np.exp(chunk_test_data['hosp_shifted'])  # Assuming the target column is 'hosp_shifted'\n",
    "        \n",
    "        if model_type == 'RF':\n",
    "            # Flatten the target variable if needed\n",
    "            y_train = np.ravel(y_train)\n",
    "\n",
    "        # Fit the model with the best parameters\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Transform back\n",
    "        initial_value = hosp_virus.loc[chunk_train_data.last_valid_index(), 'hosp_shifted']\n",
    "        y_pred = initial_value + y_pred.cumsum()\n",
    "        y_pred = np.exp(y_pred)\n",
    "        # Calculate MAPE\n",
    "        mape = mean_absolute_error(y_test, y_pred) / abs(y_test.mean()) * 100\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "        # Append to test_mapes list\n",
    "        test_mapes.append(mape)\n",
    "        test_maes.append(mae)\n",
    "    \n",
    "    return(test_mapes, test_maes)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose between ['classical', 'weighted_linear', 'weighted_squared', 'blocked', 'blocked_small', 'k_fold', 'no_cv']\n",
    "global CV_CHOICES\n",
    "CV_CHOICES = ['classical', 'weighted_linear', 'weighted_squared', 'blocked', 'blocked_small', 'k_fold', 'no_cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_and_evaluate_all():\n",
    "    # Define all possible combinations of parameters\n",
    "    is_autoregressive_options = [True, False]\n",
    "    model_type_options = ['XGB','RF']\n",
    "    cv_type_options = ['blocked_small']   # for paper only focus on classical ts cv\n",
    "    is_cross_modal = [False]\n",
    "    \n",
    "    total_combinations = len(is_autoregressive_options) * len(model_type_options) * len(cv_type_options)\n",
    "    current_combination = 0\n",
    "\n",
    "    for is_autoregressive in is_autoregressive_options:\n",
    "        for model_type in model_type_options:\n",
    "            for cv_type in cv_type_options:\n",
    "                current_combination += 1\n",
    "                print(f\"[{current_combination}/{total_combinations}] Optimizing and evaluating for: is_autoregressive={is_autoregressive}, model_type={model_type}, cv_type={cv_type}\")\n",
    "                \n",
    "                # Optimize hyperparameters\n",
    "                optimize_chunks(is_autoregressive=is_autoregressive, model_type=model_type, cv_type=cv_type)\n",
    "\n",
    "                # Evaluate model\n",
    "                test_mapes, test_maes = calculate_test_mapes(best_chunk_params, is_autoregressive=is_autoregressive, model_type=model_type, cv_type=cv_type, is_cross_modal=is_cross_modal)\n",
    "                \n",
    "\n",
    "                average_mape = np.round(np.mean(test_mapes),2)\n",
    "                average_mae = np.round(np.mean(test_maes),2)\n",
    "                median_mape = np.round(np.median(test_mapes),2)\n",
    "                median_mae = np.round(np.median(test_maes),2)\n",
    "                CI_mape = calculate_CI(test_mapes)\n",
    "                CI_mae = calculate_CI(test_maes)\n",
    "                \n",
    "                output_name_metrics = \"../output/\"+model_type+\"_\"+str(is_autoregressive)+\"_metrics.csv\"\n",
    "                output_name_summary = \"../output/\"+model_type+\"_\"+str(is_autoregressive)+\"_summary.csv\"\n",
    "                \n",
    "                metrics = pd.DataFrame({\"MAPE\":test_mapes, \"MAE\":test_maes})\n",
    "                summary = pd.DataFrame({\"Mean_MAPE\":average_mape, \"Median_MAPE\":median_mape, \"CI_low_MAPE\":CI_mape[0],\"CI_up_MAPE\":CI_mape[1], \n",
    "                                        \"Mean_MAE\": average_mae, \"Median_MAE\":median_mae, \"CI_low_MAE\":CI_mae[0],\"CI_up_MAE\":CI_mae[1] }, index=[0])\n",
    "                metrics.to_csv(output_name_metrics,index=False)\n",
    "                summary.to_csv(output_name_summary,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_evaluate_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for cross-modal only\n",
    "\n",
    "def optimize_and_evaluate_cross_modal():\n",
    "    # Define all possible combinations of parameters\n",
    "    is_autoregressive_options = [False]\n",
    "    model_type_options = ['XGB','RF']\n",
    "    cv_type_options = ['blocked_small']   # for paper only focus on classical ts cv\n",
    "    is_cross_modal = [True]\n",
    "    \n",
    "    total_combinations = len(is_autoregressive_options) * len(model_type_options) * len(cv_type_options)\n",
    "    current_combination = 0\n",
    "\n",
    "    for is_autoregressive in is_autoregressive_options:\n",
    "        for model_type in model_type_options:\n",
    "            for cv_type in cv_type_options:\n",
    "                current_combination += 1\n",
    "                print(f\"[{current_combination}/{total_combinations}] Optimizing and evaluating for: is_autoregressive={is_autoregressive}, model_type={model_type}, cv_type={cv_type}\")\n",
    "                \n",
    "                # Optimize hyperparameters\n",
    "                optimize_chunks(is_autoregressive=is_autoregressive, model_type=model_type, cv_type=cv_type)\n",
    "\n",
    "                # Evaluate model\n",
    "                test_mapes, test_maes = calculate_test_mapes(best_chunk_params, is_autoregressive=is_autoregressive, model_type=model_type, cv_type=cv_type, is_cross_modal=is_cross_modal)\n",
    "                \n",
    "\n",
    "                average_mape = np.round(np.mean(test_mapes),2)\n",
    "                average_mae = np.round(np.mean(test_maes),2)\n",
    "                median_mape = np.round(np.median(test_mapes),2)\n",
    "                median_mae = np.round(np.median(test_maes),2)\n",
    "                CI_mape = calculate_CI(test_mapes)\n",
    "                CI_mae = calculate_CI(test_maes)\n",
    "                \n",
    "                output_name_metrics = \"../output/\"+model_type+\"_cross_modal_metrics.csv\"\n",
    "                output_name_summary = \"../output/\"+model_type+\"_cross_modal_summary.csv\"\n",
    "                \n",
    "                metrics = pd.DataFrame({\"MAPE\":test_mapes, \"MAE\":test_maes})\n",
    "                summary = pd.DataFrame({\"Mean_MAPE\":average_mape, \"Median_MAPE\":median_mape, \"CI_low_MAPE\":CI_mape[0],\"CI_up_MAPE\":CI_mape[1], \n",
    "                                        \"Mean_MAE\": average_mae, \"Median_MAE\":median_mae, \"CI_low_MAE\":CI_mae[0],\"CI_up_MAE\":CI_mae[1] }, index=[0])\n",
    "                metrics.to_csv(output_name_metrics,index=False)\n",
    "                summary.to_csv(output_name_summary,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_evaluate_cross_modal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f23ace84943ce80e250d3e34fbae8e2514d638635126369f891a764c405e7d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
