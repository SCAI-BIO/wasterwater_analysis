{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e8534c3faf523b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:35:03.615089Z",
     "start_time": "2024-07-30T19:35:03.606675Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from xgboost import XGBRegressor\n",
    "from scipy import stats\n",
    "from utils import calculate_CI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5072a352d8ed7f",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2deaee4c9073fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:35:03.842332Z",
     "start_time": "2024-07-30T19:35:03.833310Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of states with complete data\n",
    "global STATES_LIST\n",
    "STATES_LIST = ['BE','BW','BY','HH','SN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ba5ab35f5d6f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:35:03.945514Z",
     "start_time": "2024-07-30T19:35:03.871583Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import hospitalization data\n",
    "hosp = pd.read_csv(\"../data/COVID_hosp.csv\")\n",
    "hosp = hosp[(hosp['geography'] == 'DE') | (hosp['geography'].isin(['DE.' + state for state in STATES_LIST]))]\n",
    "hosp = hosp[['date', 'total', 'geography']]\n",
    "hosp = hosp.rename(columns={'total': 'hosp', 'geography': 'state'})\n",
    "hosp['state'] = hosp['state'].str.replace('DE.', '', regex=False)\n",
    "hosp['date'] = pd.to_datetime(hosp['date'])\n",
    "hosp = hosp.set_index('date', drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e2b8c120edcfe",
   "metadata": {},
   "source": [
    "#### Tranform numbers to incedence oer 100.000 using state population numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b2c5d7aaae48c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:35:04.076968Z",
     "start_time": "2024-07-30T19:35:03.945514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using numbers from statista (31.12.2022), numbers are in 1000\n",
    "# https://de.statista.com/statistik/daten/studie/71085/umfrage/verteilung-der-einwohnerzahl-nach-bundeslaendern/\n",
    "\n",
    "state_populations = {\n",
    "    \"BE\": 3755,\n",
    "    \"BW\": 11280,\n",
    "    \"BY\": 13369,\n",
    "    \"HH\": 1892,\n",
    "    \"SN\": 4086,\n",
    "    \"DE\": 84357,\n",
    "}\n",
    "\n",
    "def calculate_incidence(row):\n",
    "    state = row['state']\n",
    "    hosp_value = row['hosp']\n",
    "    population = state_populations.get(state)\n",
    "    return hosp_value / (population / 100) # Since state population numbers are in 1000, division by 100 is used to get incidence in 100.000\n",
    "\n",
    "hosp['hosp'] = hosp.apply(calculate_incidence, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57a24a96974e10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:35:45.833715Z",
     "start_time": "2024-07-30T19:35:04.101416Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import virus load data (interpolated)\n",
    "virus = pd.DataFrame()\n",
    "\n",
    "# states\n",
    "for state in STATES_LIST:\n",
    "    temp = pd.read_excel(\"../data/amelag_einzelstandorte.xlsx\")\n",
    "    temp = temp[temp['bundesland'] == state][[\"datum\", \"loess_vorhersage\", \"einwohner\", \"bundesland\"]].dropna()\n",
    "    temp = temp.rename(columns={'datum': 'date', 'loess_vorhersage': 'virus', 'einwohner': 'residents', \"bundesland\": \"state\"})\n",
    "    temp['date'] = pd.to_datetime(temp['date'])\n",
    "\n",
    "    # Weight average by number of residents like done for all of Germany in https://github.com/robert-koch-institut/Abwassersurveillance_AMELAG \n",
    "    # Compute weighted mean\n",
    "    weighted_mean = lambda x: np.average(x, weights=temp.loc[x.index, 'residents'])\n",
    "    temp = temp.groupby('date').agg({'virus': weighted_mean})\n",
    "    temp['state'] = state\n",
    "    virus = pd.concat([virus, temp])\n",
    "    \n",
    "# DE\n",
    "temp = pd.read_excel(\"../data/amelag_aggregierte_kurve.xlsx\")\n",
    "temp = temp[[\"datum\", \"loess_vorhersage\"]].dropna()\n",
    "temp = temp.rename(columns={'datum': 'date', 'loess_vorhersage': 'virus'})\n",
    "temp['date'] = pd.to_datetime(temp['date'])\n",
    "temp['state'] = 'DE'\n",
    "temp = temp.set_index('date', drop=True)\n",
    "virus = pd.concat([virus, temp])\n",
    "\n",
    "\n",
    "virus = virus.reset_index()\n",
    "virus = virus.set_index('date', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8fb01b35ecd2d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:35:45.848343Z",
     "start_time": "2024-07-30T19:35:45.833715Z"
    }
   },
   "outputs": [],
   "source": [
    "# Joining the two DataFrames on the 'date' and 'state' column\n",
    "hosp_virus = pd.merge(virus, hosp, on=['date', 'state'], how='inner')\n",
    "# Sorting by index\n",
    "hosp_virus = hosp_virus.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcfe51649171382",
   "metadata": {},
   "source": [
    "## Log transform and differentiation per state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ac3231e2e4e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:35:45.899166Z",
     "start_time": "2024-07-30T19:35:45.864048Z"
    }
   },
   "outputs": [],
   "source": [
    "# Log transform the 'hosp' column\n",
    "hosp_virus['hosp'] = np.log(hosp_virus['hosp'])\n",
    "\n",
    "# Define a function to apply shift(-7) within each group\n",
    "def shift_within_group(group):\n",
    "    group['hosp_shifted'] = group['hosp'].shift(-7)\n",
    "    group['hosp_diff'] = group['hosp'].diff()\n",
    "    group['hosp_diff_shifted'] = group['hosp_diff'].shift(-7)\n",
    "    return group\n",
    "\n",
    "# Group by 'state'\n",
    "grouped = hosp_virus.groupby('state')\n",
    "\n",
    "# Apply the shift and difference transformations within each state group\n",
    "hosp_virus = grouped.apply(shift_within_group)\n",
    "\n",
    "hosp_virus = hosp_virus.rename(columns={'state': 'state_temp'})\n",
    "hosp_virus = hosp_virus.reset_index()\n",
    "hosp_virus = hosp_virus.drop(hosp_virus.columns[0], axis=1)\n",
    "hosp_virus = hosp_virus.set_index('date', drop=True)\n",
    "hosp_virus = hosp_virus.rename(columns={'state_temp': 'state'})\n",
    "\n",
    "# Drop rows with NaN values\n",
    "hosp_virus = hosp_virus.dropna()\n",
    "\n",
    "# Sort by date\n",
    "hosp_virus = hosp_virus.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd3ec3c9443b4ce",
   "metadata": {},
   "source": [
    "# Define and use chunking method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf67ca55597438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:35:45.936609Z",
     "start_time": "2024-07-30T19:35:45.921469Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dataframe(df, train_size, test_size, shift_size):\n",
    "    \"\"\"\n",
    "    Split a given dataframe into smaller chunks with a constant train and test size.\n",
    "    The chunk window is moved forward by a specified shift value.\n",
    "    If the last test part of a chunk does not have the full test length, the chunk is discarded.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame, the dataframe to be split\n",
    "    - train_size: int, the length of the train window\n",
    "    - test_size: int, the length of the test window\n",
    "    - shift_size: int, the value by which the chunk window moves forward\n",
    "\n",
    "    Returns:\n",
    "    - chunks: list of tuples, each tuple contains a train window and its corresponding test window\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    # Determine the total length of the dataframe\n",
    "    total_length = len(df)\n",
    "\n",
    "    # Start index for the current chunk\n",
    "    start_index = 0\n",
    "\n",
    "    while start_index + train_size + test_size <= total_length:\n",
    "        # Extract the train and test data for the current chunk\n",
    "        train_data = df.iloc[start_index:start_index + train_size]\n",
    "        test_data = df.iloc[start_index + train_size:start_index + train_size + test_size]\n",
    "\n",
    "        # Add the train-test pair to the chunks list\n",
    "        chunks.append((train_data, test_data))\n",
    "\n",
    "        # Move the chunk window forward\n",
    "        start_index += shift_size\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e5a70d8ec743a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:35:45.965702Z",
     "start_time": "2024-07-30T19:35:45.936609Z"
    }
   },
   "outputs": [],
   "source": [
    "chunks = split_dataframe(hosp_virus, 70*6, 7*6, 7*6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1b2e5c75aa16f",
   "metadata": {},
   "source": [
    "# Cross validation and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555df9b57fb1366b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:35:45.989757Z",
     "start_time": "2024-07-30T19:35:45.965702Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial, chunk_train_data, is_autoregressive: bool, model_type: str, cv_type: str) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for Optuna.\n",
    "    Optimizes Hyperparameters for XBGoost or Random Forest model for a given Cross-Validation method.\n",
    "    Data can be autoregressive or also include the waste water data.\n",
    "    \n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): Optuna's trial object used for optimization.\n",
    "        chunk_train_data (pd.DataFrame): Training data chunk for the current iteration.\n",
    "        is_autoregressive (bool): Flag indicating whether the data is autoregressive.\n",
    "        model_type (str): Type of model to be optimized, either 'XGB' for XGBoost or 'RF' for Random Forest.\n",
    "        cv_type (str): Type of cross-validation method to be used.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean MAPE (Mean Absolute Percentage Error) across all cross-validation folds for the given hyperparameters.\n",
    "    \"\"\"\n",
    "    # Define feature and target columns based on is_autoregressive flag\n",
    "    TARGET_COLUMN = ['hosp_diff_shifted']\n",
    "    if is_autoregressive:\n",
    "        FEATURE_COLUMNS = ['hosp_diff']\n",
    "    else:\n",
    "        FEATURE_COLUMNS = ['virus', 'hosp_diff']\n",
    "\n",
    "\n",
    "    # Define model parameters based on model_type\n",
    "    if model_type == 'XGB':\n",
    "        params = {\n",
    "            'n_estimators': 200,\n",
    "            'random_state': 42,\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 12),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 10, log=True),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 10, log=True)\n",
    "        }\n",
    "        model = XGBRegressor(**params)\n",
    "    elif model_type == 'RF':\n",
    "        params = {\n",
    "            'n_jobs': -1, # Utilize all available CPU cores\n",
    "            'n_estimators': 200,\n",
    "            'random_state': 42,\n",
    "            'min_samples_split': trial.suggest_categorical('min_samples_split', [2, 4, 8, 16]),\n",
    "            'min_samples_leaf': trial.suggest_categorical('min_samples_leaf', [1, 2, 4]),\n",
    "        }\n",
    "        model = RandomForestRegressor(**params)\n",
    "        \n",
    "    # Define CV-method to be used\n",
    "    if cv_type == 'classical':\n",
    "        splitter = TimeSeriesSplit(n_splits=5, test_size=7*8)\n",
    "    if cv_type == 'blocked' or 'blocked_small':\n",
    "        chunk_splits = split_dataframe(chunk_train_data, int(len(chunk_train_data)*0.2), int(len(chunk_train_data)*0.2), int(len(chunk_train_data)*0.1))\n",
    "    if cv_type == 'weighted_linear' or cv_type == 'weighted_squared':\n",
    "        splitter = TimeSeriesSplit(n_splits=5, test_size=7*8)\n",
    "        weights = []\n",
    "    if cv_type == 'k_fold':\n",
    "        splitter = KFold(n_splits=5)\n",
    "    if cv_type == 'no_cv':\n",
    "        train_size = int(len(chunk_train_data) * 0.8)\n",
    "        train_no_cv = chunk_train_data.iloc[:train_size]\n",
    "        test_no_cv = chunk_train_data.iloc[train_size:]\n",
    "        chunk_splits = [(train_no_cv, test_no_cv)]\n",
    "\n",
    "    # Calculate average MAPE over all CV-folds (splits)\n",
    "    split_mapes = []\n",
    "    split_maes = []\n",
    "    \n",
    "    if cv_type in ['classical', 'weighted_linear', 'weighted_squared', 'k_fold']:\n",
    "        for split_number, (train_idx, val_idx) in enumerate(splitter.split(chunk_train_data)):\n",
    "            train_data = chunk_train_data.iloc[train_idx]\n",
    "            val_data = chunk_train_data.iloc[val_idx]\n",
    "    \n",
    "            X_train, y_train = train_data[FEATURE_COLUMNS], train_data[TARGET_COLUMN]\n",
    "    \n",
    "            y_train_df = y_train.copy()\n",
    "            if model_type == 'RF':\n",
    "                # Flatten the target variable if needed\n",
    "                y_train = np.ravel(y_train)\n",
    "    \n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Do predictions for every included state\n",
    "            state_mapes = []\n",
    "            state_maes = []\n",
    "            for state in STATES_LIST + ['DE']:\n",
    "                state_train_data = train_data[train_data['state'] == state]\n",
    "                state_val_data = val_data[val_data['state'] == state]\n",
    "                \n",
    "                X_test, y_test = state_val_data[FEATURE_COLUMNS], np.exp(state_val_data['hosp_shifted'])\n",
    "            \n",
    "                y_pred = model.predict(X_test)\n",
    "        \n",
    "                # Transform back\n",
    "                state_hosp_virus = hosp_virus[hosp_virus['state'] == state]\n",
    "                initial_value = state_hosp_virus.loc[state_train_data.last_valid_index(), 'hosp_shifted']\n",
    "\n",
    "                y_pred = initial_value + y_pred.cumsum()\n",
    "                y_pred = np.exp(y_pred)\n",
    "    \n",
    "                mape = mean_absolute_error(y_test, y_pred) / abs(y_test.mean()) * 100\n",
    "                state_mapes.append(mape)\n",
    "                mae = mean_absolute_error(y_test, y_pred) \n",
    "                state_maes.append(mae)\n",
    "                \n",
    "            split_mapes.append(np.mean(state_mapes))\n",
    "            split_maes.append(np.mean(state_maes))\n",
    "    \n",
    "            if cv_type == 'weighted_linear' or cv_type == 'weighted_squared':\n",
    "                weights.append(len(train_idx))\n",
    "    \n",
    "    if cv_type in ['blocked', 'blocked_small', 'no_cv']:\n",
    "        for split_number, (train_chunk, test_chunk) in enumerate(chunk_splits):\n",
    "            X_train, y_train = train_chunk[FEATURE_COLUMNS], train_chunk[TARGET_COLUMN]\n",
    "            X_test, y_test = test_chunk[FEATURE_COLUMNS], np.exp(test_chunk['hosp_shifted'])\n",
    "\n",
    "            y_train_df = y_train.copy()\n",
    "            if model_type == 'RF':\n",
    "                # Flatten the target variable if needed\n",
    "                y_train = np.ravel(y_train)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Do predictions for every included state\n",
    "            state_mapes = []\n",
    "            state_maes = []\n",
    "            for state in STATES_LIST + ['DE']:\n",
    "                state_train_data = train_chunk[train_chunk['state'] == state]\n",
    "                state_val_data = test_chunk[test_chunk['state'] == state]\n",
    "\n",
    "                X_test, y_test = state_val_data[FEATURE_COLUMNS], np.exp(state_val_data['hosp_shifted'])\n",
    "\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                # Transform back\n",
    "                state_hosp_virus = hosp_virus[hosp_virus['state'] == state]\n",
    "                initial_value = state_hosp_virus.loc[state_train_data.last_valid_index(), 'hosp_shifted']\n",
    "\n",
    "                y_pred = initial_value + y_pred.cumsum()\n",
    "                y_pred = np.exp(y_pred)\n",
    "\n",
    "                mape = mean_absolute_error(y_test, y_pred) / abs(y_test.mean()) * 100\n",
    "                state_mapes.append(mape)\n",
    "                mae = mean_absolute_error(y_test, y_pred) \n",
    "                state_maes.append(mae)\n",
    "\n",
    "            split_mapes.append(np.mean(state_mapes))\n",
    "            split_maes.append(np.mean(state_maes))\n",
    "\n",
    "\n",
    "    # Return mean MAPE or weighted mean MAPE\n",
    "    if cv_type == 'weighted_linear':\n",
    "        linear_weights = [value / sum(weights) for value in weights]\n",
    "        return np.average(split_mapes, weights=linear_weights)\n",
    "    elif cv_type == 'weighted_squared':\n",
    "        squared_weights = [value**2 / sum([value**2 for value in weights]) for value in weights]\n",
    "        return np.average(split_mapes, weights=squared_weights)\n",
    "    else:\n",
    "        return np.mean(split_mapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7207897800ae74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:35:46.003263Z",
     "start_time": "2024-07-30T19:35:45.991279Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize_chunks(is_autoregressive: bool, model_type: str, cv_type: str) -> None:\n",
    "    \"\"\"\n",
    "    Optimizes model hyperparameters for each chunk of data.\n",
    "\n",
    "    Args:\n",
    "        is_autoregressive (bool): Flag indicating whether the data is autoregressive.\n",
    "        model_type (str): Type of model to be optimized, either 'XGB' for XGBoost or 'RF' for Random Forest.\n",
    "        cv_type (str): Type of cross-validation method to be used.\n",
    "    \"\"\"\n",
    "    # Set the logging level of Optuna to ERROR\n",
    "    optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "    \n",
    "    # Reset the lists before starting a new optimization task\n",
    "    global best_chunk_params, cv_mapes, test_mapes, test_maes  # Declare global variables\n",
    "    best_chunk_params = []\n",
    "    cv_mapes = []\n",
    "    test_mapes = []\n",
    "    test_maes = []\n",
    "\n",
    "    for chunk_number, (chunk_train_data, chunk_test_data) in enumerate(chunks):\n",
    "        print(f'CURRENT CHUNK: {chunk_number}/{len(chunks)}')\n",
    "        \n",
    "        if model_type == 'XGB':\n",
    "            study = optuna.create_study(direction='minimize')\n",
    "        if model_type == 'RF':\n",
    "            # Define categorical parameters for Random Forest\n",
    "            param_grid = {\n",
    "                'min_samples_split': [2, 4, 8, 16],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "            }\n",
    "            study = optuna.create_study(direction='minimize', sampler=optuna.samplers.GridSampler(param_grid))\n",
    "            \n",
    "        objective_func = lambda trial: objective(trial, chunk_train_data, is_autoregressive, model_type, cv_type)\n",
    "        study.optimize(objective_func, n_trials=100) # Only used for TPE-Sampler \n",
    "\n",
    "        best_chunk_params.append(study.best_params)\n",
    "        cv_mapes.append(study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2252f24d30c1e5ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:35:46.017664Z",
     "start_time": "2024-07-30T19:35:46.003263Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_test_mapes(best_chunk_params, is_autoregressive: bool, model_type: str, cv_type: str) -> None:\n",
    "    \"\"\"\n",
    "    Calculate test MAPEs for each chunk using the best parameters found during optimization.\n",
    "\n",
    "    Args:\n",
    "        best_chunk_params (list): List of best parameters found for each chunk during optimization.\n",
    "        is_autoregressive (bool): Flag indicating whether the data is autoregressive.\n",
    "        model_type (str): Type of model to be used, either 'XGB' for XGBoost or 'RF' for Random Forest.\n",
    "    \"\"\"\n",
    "    \n",
    "    for chunk_number, (chunk_train_data, chunk_test_data) in enumerate(chunks):\n",
    "        print(f'CURRENT TEST CHUNK: {chunk_number}')\n",
    "\n",
    "        # Account for smaller train set in refit when cv method is blocked_small\n",
    "        if cv_type == 'blocked_small':\n",
    "            discard_size = int(len(chunk_train_data) * 0.8)\n",
    "            chunk_train_data = chunk_train_data.iloc[discard_size:]\n",
    "\n",
    "        best_params = best_chunk_params[chunk_number]  # Retrieve the best parameters for the current chunk\n",
    "\n",
    "        # Define feature and target columns based on is_autoregressive flag\n",
    "        TARGET_COLUMN = ['hosp_diff_shifted']\n",
    "        if is_autoregressive:\n",
    "            FEATURE_COLUMNS = ['hosp_diff']\n",
    "        else:\n",
    "            FEATURE_COLUMNS = ['virus', 'hosp_diff']\n",
    "\n",
    "        # Define model based on model_type\n",
    "        if model_type == 'XGB':\n",
    "            model = XGBRegressor(**best_params)\n",
    "        elif model_type == 'RF':\n",
    "            model = RandomForestRegressor(**best_params)\n",
    "\n",
    "        X_train = chunk_train_data[FEATURE_COLUMNS]\n",
    "        y_train = chunk_train_data[TARGET_COLUMN]\n",
    "        \n",
    "        X_test = chunk_test_data[FEATURE_COLUMNS]\n",
    "        y_test = np.exp(chunk_test_data['hosp_shifted'])  # Assuming the target column is 'hosp_shifted'\n",
    "        \n",
    "        if model_type == 'RF':\n",
    "            # Flatten the target variable if needed\n",
    "            y_train = np.ravel(y_train)\n",
    "\n",
    "        # Fit the model with the best parameters\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Do predictions for every included state\n",
    "        state_mapes = []\n",
    "        state_maes = []\n",
    "        for state in STATES_LIST + ['DE']:\n",
    "            state_train_data = chunk_train_data[chunk_train_data['state'] == state]\n",
    "            state_val_data = chunk_test_data[chunk_test_data['state'] == state]\n",
    "\n",
    "            X_test, y_test = state_val_data[FEATURE_COLUMNS], np.exp(state_val_data['hosp_shifted'])\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Transform back\n",
    "            state_hosp_virus = hosp_virus[hosp_virus['state'] == state]\n",
    "            initial_value = state_hosp_virus.loc[state_train_data.last_valid_index(), 'hosp_shifted']\n",
    "\n",
    "            y_pred = initial_value + y_pred.cumsum()\n",
    "            y_pred = np.exp(y_pred)\n",
    "\n",
    "            mape = mean_absolute_error(y_test, y_pred) / abs(y_test.mean()) * 100\n",
    "            state_mapes.append(mape)\n",
    "            mae = mean_absolute_error(y_test, y_pred) \n",
    "            state_maes.append(mae)\n",
    "\n",
    "        # Append to test_mapes list\n",
    "        test_mapes.append(np.mean(state_mapes))\n",
    "        test_maes.append(np.mean(state_maes))\n",
    "    \n",
    "    return(test_mapes, test_maes)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3d5d5056bc0a2",
   "metadata": {},
   "source": [
    "### Define CV-Methods to be used for optimization and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6847ab00dd91d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:35:46.031208Z",
     "start_time": "2024-07-30T19:35:46.021186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose between ['classical', 'weighted_linear', 'weighted_squared', 'blocked', 'blocked_small', 'k_fold', 'no_cv']\n",
    "global CV_CHOICES\n",
    "CV_CHOICES = ['classical', 'weighted_linear', 'weighted_squared', 'blocked', 'blocked_small', 'k_fold', 'no_cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e644b247e68e01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T19:35:46.045354Z",
     "start_time": "2024-07-30T19:35:46.031208Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize_and_evaluate_all():\n",
    "    # Define all possible combinations of parameters\n",
    "    is_autoregressive_options = [False]\n",
    "    model_type_options = ['RF','XGB']\n",
    "    cv_type_options = ['blocked_small']   # for paper only focus on classical ts cv\n",
    "    \n",
    "    \n",
    "    total_combinations = len(is_autoregressive_options) * len(model_type_options) * len(cv_type_options)\n",
    "    current_combination = 0\n",
    "\n",
    "    for is_autoregressive in is_autoregressive_options:\n",
    "        for model_type in model_type_options:\n",
    "            for cv_type in cv_type_options:\n",
    "                current_combination += 1\n",
    "                print(f\"[{current_combination}/{total_combinations}] Optimizing and evaluating for: is_autoregressive={is_autoregressive}, model_type={model_type}, cv_type={cv_type}\")\n",
    "                \n",
    "                # Optimize hyperparameters\n",
    "                optimize_chunks(is_autoregressive=is_autoregressive, model_type=model_type, cv_type=cv_type)\n",
    "\n",
    "                # Evaluate model\n",
    "                test_mapes, test_maes = calculate_test_mapes(best_chunk_params, is_autoregressive=is_autoregressive, model_type=model_type, cv_type=cv_type)\n",
    "                \n",
    "\n",
    "                average_mape = np.round(np.mean(test_mapes),2)\n",
    "                average_mae = np.round(np.mean(test_maes),2)\n",
    "                median_mape = np.round(np.median(test_mapes),2)\n",
    "                median_mae = np.round(np.median(test_maes),2)\n",
    "                CI_mape = calculate_CI(test_mapes)\n",
    "                CI_mae = calculate_CI(test_maes)\n",
    "                \n",
    "                output_name_metrics = \"../output/\"+model_type+\"_\"+str(is_autoregressive)+\"_regional_6_metrics.csv\"\n",
    "                output_name_summary = \"../output/\"+model_type+\"_\"+str(is_autoregressive)+\"_regional_6_summary.csv\"\n",
    "                \n",
    "                metrics = pd.DataFrame({\"MAPE\":test_mapes, \"MAE\":test_maes})\n",
    "                summary = pd.DataFrame({\"Mean_MAPE\":average_mape, \"Median_MAPE\":median_mape, \"CI_low_MAPE\":CI_mape[0],\"CI_up_MAPE\":CI_mape[1], \n",
    "                                        \"Mean_MAE\": average_mae, \"Median_MAE\":median_mae, \"CI_low_MAE\":CI_mae[0],\"CI_up_MAE\":CI_mae[1] }, index=[0])\n",
    "                metrics.to_csv(output_name_metrics,index=False)\n",
    "                summary.to_csv(output_name_summary,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1e8972",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_evaluate_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215c0fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f23ace84943ce80e250d3e34fbae8e2514d638635126369f891a764c405e7d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
